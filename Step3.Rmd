---
title: "Step3_test"
author: "Annie Kellner"
date: "2023-02-07"
output: html_document
---

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # 'echo' means R will print the code along with the results. 

#rm(list = ls()) # clear workspace

library(terra)
library(sf)
library(tidyverse)
library(data.table)
library(viridis)
library(mapview)
library(raster)
library(lubridate)

source('./Code/CEMML/Misc/character_vectors.R')
source('N:/RStor/mindyc/afccm/Climate Modeling/Software Apps/R scripts/LOCA_V2/Function_General.R')
```

# *PART ONE: User settings - choose AFB, model, scenarios, years of interest*

```{r set-directories}

dir_installation_boundaries <- "N:\\RStor\\mindyc\\afccm\\AF_CIP_ENV_Data_Phase3/Installation_Boundaries/" # site boundary shape files used for clipping

dir_output_csvs = './Results/Test' 

#dir_functions = "N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Software Apps\\R scripts\\LOCA_V2" # scripts that call in functions for analysis
```



First, let's select which of the `models` you'd like to run. Highlight the word `models` and click 'Run' at the top right (or use the shortcut *Ctrl + Enter*). The available models correspond to numbers in the vector (e.g., HADGEM2-ES is [1]). 

In the code chunk below, enter the number between the brackets ([]) that corresponds with your model of interest. Then let's see what scenarios and variables are available for that model.

```{r enter-afb-and-model}

AFB_Name <- "Homestead_ARB" # Can create a vector similar to 'models' above if desired

model <- models[2] 

model_dir <- paste("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Data\\", model, sep = "")
scenario_dirs <- list.dirs(path = paste("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Data\\", model, sep = ""), full.names = FALSE)

cat(paste("The model you have selected is", model, "and the AFB is", AFB_Name))

```

The next chunk checks what scenarios (subfolders) are contained in the 'Data' folder associated with the model of interest.

```{r results='asis'}

cat("The scenarios (subfolders) associated with this model are:", sep = '\n') # cat() basically means 'print' in this context 
cat(paste("-", scenario_dirs), sep = "\n") # names into separate lines for readability ('\n' = line)

```

*Enter the scenarios of interest below:*

```{r enter-scenarios}

baseline <- "historical"
scenario1 <- "rcp45"
scenario2 <- "rcp85"

```

Next, let's see which variables are represented in this dataset

```{r}

baseline_dir = paste("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Data\\", model, "\\", baseline, sep = "")
baseline_filenames <- list.files(baseline_dir, pattern = '.nc', full.names = FALSE) # this line might need to be adjusted if some files are .ncdf or other

variables <- unique(substr(baseline_filenames, 1,6)) # Numbers 1 and 6 refer to character indices within the file names (i.e., the first six letters of the file name)

cat("The variables in this dataset are:", sep = '\n')
cat(paste("-", variables), sep = "\n")

```

```{r enter-years-of-interest}

baseline_start_year <- 1976 # Start year of interest for the historical time period  
baseline_end_year <- 2005 # End year of interest for the historical time period

future1_start_year <- 2026 # Start year of interest for the first future time period 
future1_end_year <- 2035 # End year of interest for the first future time period 

future2_start_year <- 2046 # Start year of interest for the second future time period 
future2_end_year <- 2055 # End year of interest for the second future time period 

```

####################################################################################################
# THIS CONCLUDES THE USER SETTINGS PORTION OF THE SCRIPT
# The rest of the script should be able to be run as-is...

# PART TWO: SUBSET FILES OF INTEREST

In this step, we are creating lists of filenames that we will eventually combine into raster stacks. WHen this chunk is finished running, you can check the objects to make sure 

```{r list-filenames}

scenarios <- c(baseline, scenario1, scenario2)

baseline_yrs <- paste(seq(baseline_start_year, baseline_end_year, 1), collapse = '|')
future1_yrs <- paste(seq(future1_start_year, future1_end_year, 1), collapse = '|')
future2_yrs <- paste(seq(future2_start_year, future2_end_year, 1), collapse = '|')

files_baseline <- list() # historical
files_s1f1 <- list() # scenario1, future 1
files_s1f2 <- list() # scenario 1, future 2
files_s2f1 <- list() # scenario 2, future 1
files_s2f2 <- list() # scenario 2, future 2

# List files for Baseline Scenario

for(i in 1:length(variables)){ 
  fileNames = list.files(baseline_dir, pattern = variables[i], full.names = TRUE)
  dt = data.table(fileNames, result = grepl(baseline_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_baseline[[i]] = dt2$fileNames
}

# List files for S1F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '\\')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f1[[i]] = dt2$fileNames
  }

# List files for S1F2

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '\\')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f2[[i]] = dt2$fileNames
  }

# List files for S2F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario2, sep = '\\')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f1[[i]] = dt2$fileNames
}

# List files for S2F2

for(i in 1:length(variables)){ 
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f2[[i]] = dt2$fileNames
}

```

# PART THREE: SPATIAL CHECK

The next chunk checks that the spatial files are where they should be. The map will appear below the code chunk, and you can zoom in as needed to view small AFB's 

```{r spatial-check}

# AFB

afb_dir <- (paste(dir_installation_boundaries, AFB_Name, sep = '/'))

afbSF <- st_read(paste(afb_dir, '.shp', sep = "")) # read in using sf package first
afbSF <- sf::st_as_sf(afbSF, coords = c("longitude", "latitude"), crs = st_crs(4326)) # EPSG:4326 is lat/long

afb <- vect(afbSF) # for use with terra package

# Raster

stackBaseline_v1 <- rast(files_baseline[[1]]) # creates raster stack for first variable in files_baseline list
rx <- raster(stackBaseline_v1[[1]]) # for plotting and getting basic raster info

# Quick plot

mapview(rx, col.regions = hcl.colors(5, palette = "viridis", alpha = 0.8)) + # First argument is arbitrary (# colors in palette)
  mapview(afbSF, col.regions = "black")

```

We need to check the size of the AFB to see whether we should use the centroid or multiple cells for analysis. If the results say 'matrix array', the script will use the centroid. If it says "spatVector", it will use the spatial object (afb).

```{r decision-to-use-shp-or-centroid-for-extraction, warning=FALSE, results='asis'}

# What is the cell size of the ncdf?

rx <- rast(rx) # convert to terra object because estimation is better. Could also use raster::area()
rxCrop <- terra::crop(rx, afb)
rxCell <- cellSize(rxCrop, unit = "km")
ncdf_cellSize <- mean(values(rxCell)) # get mean cell size of ncdf

# How big is the AFB?

afb_area <- expanse(afb, unit = "km")

centroid <- afbSF %>%
  st_centroid() %>%
  st_geometry() %>%
  st_coordinates()

# add an if_else statement to tell R whether to extract raster values from matrix or shapefile

extract_obj <- afb
cell <- cellFromXY(rx, centroid) # finds the cell (index) in which the centroid is located

extract_obj <- if(afb_area < ncdf_cellSize) cell else (afb)

cat("The object used for data extraction is ", class(extract_obj)) # 'numeric' = centroid (cell number); 
                                                                   # 'spatVector' = spatial object (multiple cells)

```


```{r extract-values}

# Create lists for extracted values and vectors for extracting time (dates) from rasters
# NOTE: this part of the script could be turned into a nested looped with code like for(i in 1:length(scenarios)) and for(j in 1:length(futures)) and for(k in length(1:variables)). I would add rm(r) to the end so the rasters are removed from the environment before the next loop starts

# Historical

results_baseline <- list()

results_s1f1 <- list()
varnames_s1f1 <- list()
dates_s1f1 <- vector()

results_s1f2 <- list()
varnames_s1f2 <- list()
dates_s1f2 <- vector()



results_s2f1 <- list()
results_s2f2 <- list()

for(i in 1:length(files_baseline)){
  r = rast(files_baseline[[i]])
  times = time(r)
  vals = r[extract_obj]
  rData = nest()
  results_baseline[[i]] = vals %>%
    pivot_longer(cols = everything(),
                 names_to = "Layer",
                 names_prefix = variables[i],
                 values_to = variables[i],
                 values_drop_na = FALSE) %>%
    mutate(dates = times)
  rm(r)
}

for(i in 1:length(files_s1f1)){
  r = rast(files_s1f1[[i]])
  dates_s1f1 = time(r)
  vals = r[extract_obj]
  results_s1f1[[i]] = vals %>%
    pivot_longer(
      cols = everything(),
      names_to = "Layer",
      names_prefix = variables[i],
      values_to = variables[i],
      values_drop_na = FALSE) %>%
    mutate(dates = times)
  rm(r)
}

for(i in 1:length(files_s1f2)){ # good - use this template?
  r = rast(files_s1f2[[i]])
  dates_s1f2 = time(r)
  varnames_s1f2[[i]] = varnames(r)
  vals = r[extract_obj]
  tVals = transpose(vals)
  results_s1f2[[i]] = tVals
  rm(r)
}

t2 <- as_tibble(results_s1f2, .name_repair = "unique") 


t2 <- unnest(t) 

for(i in 1:length(files_s2f1)){
  r = rast(files_s2f1[[i]])
  times = time(r)
  vals = r[extract_obj]
  results_s2f1[[i]] = pivot_longer(results_s2f1[[i]],
                           cols = everything(),
                           names_to = "Layer",
                           names_prefix = variables[i],
                           values_to = variables[i],
                           values_drop_na = FALSE) %>%
    mutate(dates = times)
  rm(r)
}

for(i in 1:length(files_s2f2)){
  r = rast(files_s2f2[[i]])
  times = time(r)
  vals = r[extract_obj]
  results_s2f2[[i]] = pivot_longer(results_s2f2[[i]],
                           cols = everything(),
                           names_to = "Layer",
                           names_prefix = variables[i],
                           values_to = variables[i],
                           values_drop_na = FALSE) %>%
    mutate(dates = times)
  rm(r)
}

rm(files_baseline, files_s1f1, files_s1f2, files_s2f1, files_s2f2) # remove all lists from environment except results

```

```{r final-dataframes}

# Equivalent of allvaluesdf (from original script)

lists <- Filter(function(x) is(x, "list"), mget(ls()))

create_allvaluesdf <- function(df){
  tibble(lat = centroid[2],
         lon = centroid[1],
         date = 
  
}
  
s1f1 <- tibble(lat = centroid[2], # allvaluesdf historical
                   lon = centroid[1], 
                   date = times_s1f1, 
                   prcp = results_s1f1[[1]]$pr_day,
                   tmax = results_s1f1[[2]]$tasmax,
                   tmin = results_s1f1[[3]]$tasmin)



```


```{r create csvs}

# Add derived variables to dataframes

csv <- df %>%
  mutate(PPT_mm = case_when( # if raster units are "kg-m-2 -1", convert to mm (otherwise keep as is)
      str_detect(units(rx), "kg") ~ prcp*86400,
      TRUE ~ as.numeric(prcp))) %>%
  mutate(PPT_in = RasterUnitConvert(PPT_mm, "MMtoIN")) %>%
  mutate(TMaxF = case_when(
    tmax > 200 ~ RasterUnitConvert(tmax, "KtoF"), # Because only values in Kelvin would be > 200
    TRUE ~ RasterUnitConvert(tmax, "CtoF"))) %>% # assuming if temp units are not Kelvin they are Celsius
  mutate(TMaxC = case_when(
    tmax > 200 ~ RasterUnitConvert(tmax, "KtoC"),
    TRUE ~ tmax)) %>% # assuming if temp units are not Kelvin they are Celsius
  mutate(TMinF = case_when(
    tmin > 200 ~ RasterUnitConvert(tmin, "KtoF"),
    TRUE ~ RasterUnitConvert(tmin, "CtoF"))) %>% # assuming if temp units are not Kelvin they are Celsius
  mutate(TMinC = case_when(
    tmin > 200 ~ RasterUnitConvert(tmin, "KtoC"),
    TRUE ~ tmin)) %>% # assuming if temp are not K they are C
  mutate(TMeanF = (TMaxF + TMinF)/2) %>%
  mutate(TmeanC = (TMaxC + TMinC)/2) %>%
  mutate(GDDF = RasterGDD(TMinF, TMaxF, 50, 86)) %>% # values of 50 and 86 come from original LOCASummmarize.R script
  mutate(hotdays = Rasterhotdays(TMaxC)) %>%
  mutate(colddays = Rastercolddays(TMinC)) %>%
  mutate(wetdays = Rasterwetdays(PPT_mm)) %>%
  mutate(ftDays = RasterFTdays(TMaxC, TMinC))

write.csv(csv, )

```



```{r check-results}

# Bring in results 

avdf <- read_csv('./Results/allvaluesdf.Rds')


piv2$Compare <- results$PPT_mm

piv2$Diff <- piv2$PPT_mm - piv2$Compare

sum(piv2$Diff)

```












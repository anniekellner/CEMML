---
title: "Step3_test"
author: "Annie Kellner"
date: "2023-02-07"
output: html_document
---

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # 'echo' means R will print the code along with the results. 

library(terra)
library(sf)
library(tidyverse)
library(data.table)
library(viridis)
library(mapview)
library(lubridate)
library(tmap)
library(tmaptools)
library(leaflet)

# The 'source' function reads the following scripts and places associated functions into the global environment

source('./Code/Misc/character_vectors.R')
source('./Code/CEMML/Functions/Function_General.R')
source('./Code/Step3Functions.R')
source('./Code/Stars_HowTo/shift_longitude.R')

```

```{r clear-workspace}

#rm(list = ls())

```


*PART ONE: User settings - choose AFB, AFB manager, model, scenarios, years of interest*

If the AFB is managed by the Navy, set Navy = TRUE
If the AFB is managed by the Air Force, set Navy = FALSE

```{r set-directories}

# Is the AFB managed by the Navy?

Navy = TRUE


dir_installation_boundaries <- ifelse(Navy == FALSE, # site boundary (shapefile used for clipping)
                                       "N:\\RStor\\mindyc\\afccm\\AF_CIP_ENV_Data_Phase3/Installation_Boundaries/",
                                      "N:\\RStor\\mindyc\\afccm\\AF_CIP_ENV_Data_Phase3/Installation_Boundaries/NAVY/")
                                      

dir_output_csvs = './Results/Test/Indianhead_Proxy/' 

```

First, let's select which base (AFB) and which of the `models` you'd like to run. To choose from a list, highlight the word `AFB_Name` and/or `models` (including the ``) and click 'Run' at the top right (or use the shortcut *Ctrl + Enter*). The available models correspond to numbers in the vector (e.g., HADGEM2-ES is [1]). 

In the code chunk below, enter the number between the brackets ([]) that corresponds with your model of interest. Then let's see what scenarios and variables are available for that model.

```{r enter-afb-and-model}

AFB_Name <- AFB_Name[4] 

model <- models[6] 

cat(paste("The model you have selected is", model, "and the AFB is", AFB_Name))

```

The next chunk checks what scenarios (subfolders) are contained in the 'Data' folder associated with the model of interest.

```{r results='asis'}

# Directories

model_dir <- paste("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Data\\", model, sep = "") 


scenario_dirs <- list.dirs(path = model_dir, full.names = FALSE) 

cat("The scenarios (subfolders) associated with this model are:", sep = '\n') # cat() basically means 'print' in this context 
cat(paste("-", scenario_dirs), sep = "\n") # names into separate lines for readability ('\n' = line)

```

*Enter the scenarios of interest below:*

```{r enter-scenarios}

baseline <- "historical"
scenario1 <- "ssp245"
scenario2 <- "ssp585"

```

Next, let's see which variables are represented in this dataset

```{r}

baseline_dir = paste(model_dir, baseline, sep = "/")
baseline_filenames <- list.files(baseline_dir, pattern = '.nc', full.names = FALSE) # this line might need to be adjusted if some files are .ncdf or other

variables <- unique(substr(baseline_filenames, 1,6)) # Numbers 1 and 6 refer to character indices within the file names (i.e., the first six letters of the file name)

cat("The variables in this dataset are:", sep = '\n')
cat(paste("-", variables), sep = "\n")

```

Of the variables listed, which do you want to include in your analysis? If you want to include all variables, you can skip this step. I have commented this out as a default but if you are choosing among variables, you will need to keep it in. 

```{r}

variables <- c("pr_day", "tasmax", "tasmin", "hurs_d", "sfcWin") # an example of how you would choose among the variables if desired

```


```{r enter-years-of-interest}

baseline_start_year <- 1985 # Start year of interest for the historical time period  
baseline_end_year <- 2014 # End year of interest for the historical time period

future1_start_year <- 2021 # Start year of interest for the first future time period 
future1_end_year <- 2050 # End year of interest for the first future time period 

future2_start_year <- 2051 # Start year of interest for the second future time period 
future2_end_year <- 2080 # End year of interest for the second future time period 

years <- c(baseline_start_year, 
           baseline_end_year, 
           future1_start_year, 
           future1_end_year, 
           future2_start_year, 
           future2_end_year)
```

####################################################################################################
# THIS CONCLUDES THE USER SETTINGS PORTION OF THE SCRIPT
# The rest of the script should be able to be run as-is...

# PART TWO: SUBSET FILES OF INTEREST

In this step, we are creating lists of filenames that we will eventually combine into raster stacks. When this chunk is finished running, you can check the objects to make sure 

```{r list-filenames}

scenarios <- c(baseline, scenario1, scenario2)

baseline_yrs <- paste(seq(baseline_start_year, baseline_end_year, 1), collapse = '|')
future1_yrs <- paste(seq(future1_start_year, future1_end_year, 1), collapse = '|')
future2_yrs <- paste(seq(future2_start_year, future2_end_year, 1), collapse = '|')

files_baseline <- list() # historical
files_s1f1 <- list() # scenario1, future 1
files_s1f2 <- list() # scenario 1, future 2
files_s2f1 <- list() # scenario 2, future 1
files_s2f2 <- list() # scenario 2, future 2

# List files for Baseline Scenario

for(i in 1:length(variables)){ 
  fileNames = list.files(baseline_dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(baseline_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_baseline[[i]] = dt2$fileNames
}

# List files for S1F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f1[[i]] = dt2$fileNames
  }

# List files for S1F2

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f2[[i]] = dt2$fileNames
  }

# List files for S2F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario2, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f1[[i]] = dt2$fileNames
}

# List files for S2F2

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario2, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f2[[i]] = dt2$fileNames
}

```

# PART THREE: SPATIAL CHECK

The next chunk checks that the spatial files are where they should be. The map will appear below the code chunk, and you can zoom in as needed to view small AFB's. You can press the 'show in new window' button (the white square with an arrow) to open the map in another page. You may have to wait a few moments for the map to produce. 

```{r spatial-check}

# Raster

stackX <- rast(files_s1f1[[1]]) # creates raster stack for first variable in s1f1 list (using s1f1 because has fewer files than baseline)

random_raster <- as.numeric(sample(1:nlyr(stackX), 1)) # get a random raster layer from StackX to use as an example
rx <- stackX[[random_raster]]

# AFB 

afb_dir <- (paste(dir_installation_boundaries, AFB_Name, sep = '/'))
afb_filename <- paste(afb_dir, '.shp', sep = "")

afbSF <- st_read(afb_filename) # Because plotting package (tmap) doesn't take terra vectors

afbSF <- if (xmax(rx) == 360 & xmax(afb) < 0) {st_shift_longitude(afbSF)} else afbSF # adjustment in case ncdf lon scale is 0-360

afb <- vect(afbSF)

# Crop single raster to AFB shape

rxCrop <- terra::crop(rx, afb, snap = "out") # snap = "out" shows all cells that overlap AFB polygon

# Quick interactive plot

#tmap_mode('view')

map <- tm_shape(as(rxCrop, "Raster")) + 
  tm_raster() + 
  tm_shape(afbSF) + 
  tm_borders()

lf <- tmap_leaflet(map)
lf

```

We should check the size of the AFB to see whether we should use its centroid or spatial extent to extract the data we need. If the AFB fits within a single NetCDF cell, it will be much faster to extract data using a single point.  

If the output of the following code chunk says 'numeric', the script will use the centroid. If it says "spatVector", it will use the spatial object (afb).

```{r decision-to-use-shp-or-centroid-for-extraction, warning=FALSE, results='asis'}

# What is the cell size of the ncdf?

rxCellSize <- cellSize(rxCrop, unit = "km") # size of the cropped raster
ncdf_cellSize <- mean(values(rxCellSize)) # get mean cell size of ncdf

# How big is the AFB?

afb_area <- expanse(afb, unit = "km")

centroid <- geom(centroids(afb, inside = TRUE)) # inside = TRUE guarantees that the centroid is contained within the boundaries of the AFB (in some cases, depending on the shape of the AFB, the true centroid may be outside)
centroid_matrix <- cbind(centroid[3], centroid[4]) # used to find cell index. [3] and [4] refer to the values in the 'centroid' object. If a downstream error occurs, make sure these values are actually x and y.

# add an if_else statement to tell R whether to extract raster values from matrix or shapefile

extract_obj <- afb
cell <- terra::cellFromXY(rx, centroid_matrix) # finds the cell (index) in which the centroid is located

extract_obj <- if(afb_area < ncdf_cellSize) cell else (afb)

afbExt <- ext(afb)

cat("The object used for data extraction is ", class(extract_obj)) 

# Interpreting output: 
  
  #'numeric' = centroid (cell number) - this means the AFB did not extend beyond a single cell, so the centroid will be used for data extraction
  
  #'spatVector' = spatial object (multiple cells) - this means the AFB spanned multiple cells, so the polygon will be used for data extraction

```


```{r extract-values}
# Create lists for extracted values and vectors for extracting time (dates) from rasters

# NOTE: when it comes time to run a base that is large enough to span several raster cells, will need to amend this section. 
  # Will need to write a function into Functions.R that extracts data differently depending on whether we're using the centroid (point) or a spatial object (vect). terra::extract() could do either without writing a separate function, but it takes a lot longer to run for point data

# Historical

results_baseline <- list()
results_s1f1 <- list()
results_s1f2 <- list()
results_s2f1 <- list()
results_s2f2 <- list()
```


```{r extract-values}

for(i in 1:length(files_baseline)){
  r = rast(files_baseline[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = values(crop(r, ext(afb)))
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) # do not use purrr:transpose
  colnames(tVals) = varName
  results_baseline[[i]] = tVals
  names(results_baseline)[i] = varName
  results_baseline[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s1f1)){
  r = rast(files_s1f1[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s1f1[[i]] = tVals
  names(results_s1f1)[i] = varName
  results_s1f1[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s1f2)){
  r = rast(files_s1f2[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s1f2[[i]] = tVals
  names(results_s1f2)[i] <- varName
  results_s1f2[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s2f1)){
  r = rast(files_s2f1[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s2f1[[i]] = tVals
  names(results_s2f1)[i] <- varName
  results_s2f1[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s2f2)){
  r = rast(files_s2f2[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s2f2[[i]] = tVals
  names(results_s2f2)[i] <- varName
  results_s2f2[[i]]$date = times
  rm(r)
}

rm(list = ls(pattern = "^files")) # remove lists from the environment that aren't results

```

```{r allvaluesdf-dataframes, message=FALSE}

# Equivalent of allvaluesdf (from original script)

lists <- Filter(function(x) is(x, "list"), mget(ls())) # reads in all remaining lists
nested_tibbles <- as_tibble(lists) # tibbles are tidy objects

avdf <- list() # results mimic allvaluesdf objects created by original script, with the exception of the 'time' column

for(i in 1:length(nested_tibbles)){
  sf_combo_nested = flatten(nested_tibbles[i]) # scenario-future combo
  sf = flatten(sf_combo_nested)
  tbl = tibble(lat = centroid_matrix[2], # will need to be changed when spatial objects are used for extraction
               lon = centroid_matrix[1],
               date = pluck(sf_combo_nested, 1, "date")) # pulls date column from first sublist
  sf_tbl = sf %>%
    as_tibble(sf, validate = NULL, .name_repair = "unique") %>% # validate argument necessary for running on cluster
    unnest(cols = where(is.numeric)) %>%
    dplyr::select(!(contains("date"))) %>%
    bind_cols(tbl) %>%
    dplyr::select(lat, 
                  lon, 
                  contains("max"), 
                  contains("min"), 
                  contains("pr"),
                  any_of("hurs"),
                  any_of("sfcWind"),
                  date) %>%
    rename(c(tmax = tasmax,
             tmin = tasmin,
             prcp = pr))
  
  avdf[[i]] = sf_tbl
  names(avdf)[i] = names(nested_tibbles[i]) 
}    
 
```


```{r add-derived-vars, message=FALSE}

# Add derived variables to dataframes
# All values from from LOCA_summarize.R script
# write to csv

AllDays <- list() # for MonthSum section

for(i in 1:length(avdf)){
  csv = avdf[[i]]
  csv = csv %>%
    mutate(PPT_mm = case_when( # if raster units are "kg-m-2 -1", convert to mm (otherwise keep as is)
      str_detect(units(rx), "kg") ~ prcp*86400,
      TRUE ~ as.numeric(prcp))) %>%
    mutate(PPT_in = RasterUnitConvert(PPT_mm, "MMtoIN")) %>%
    mutate(TMaxF = case_when(
      tmax > 200 ~ RasterUnitConvert(tmax, "KtoF"), # Because only values in Kelvin would be > 200
      TRUE ~ RasterUnitConvert(tmax, "CtoF"))) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMaxC = case_when(
      tmax > 200 ~ RasterUnitConvert(tmax, "KtoC"),
      TRUE ~ tmax)) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMinF = case_when(
      tmin > 200 ~ RasterUnitConvert(tmin, "KtoF"),
      TRUE ~ RasterUnitConvert(tmin, "CtoF"))) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMinC = case_when(
      tmin > 200 ~ RasterUnitConvert(tmin, "KtoC"),
      TRUE ~ tmin)) %>% # assuming if temp are not K they are C
    mutate(TMeanF = (TMaxF + TMinF)/2) %>%
    mutate(TmeanC = (TMaxC + TMinC)/2) %>%
    mutate(GDDF = RasterGDD(TMinF, TMaxF, 50, 86)) %>% 
    mutate(hotdays = Rasterhotdays(TMaxC, hottemp = 32.2)) %>%
    mutate(colddays = Rastercolddays(TMinC, coldtemp = 0)) %>%
    mutate(wetdays = Rasterwetdays(PPT_mm, wetprecip = 50.8)) %>%
    mutate(drydays = Rasterdrydays(PPT_mm, dryprecip = 2.54)) %>%
    mutate(ftdays = RasterFTdays(TMaxC, TMinC, freezethresh = -2.2, thawthresh = 1.2)) 
    
  csv_scenario = if (str_detect(names(avdf[i]), "baseline")){
    "historical"
  } else if(str_detect(names(avdf[i]),"s1")) {
    scenario1
  } else {
    scenario2
  }
  csv_years = if (str_detect(names(avdf[i]), "f1")){
    paste(future1_start_year, "-",future1_end_year)
  } else if (str_detect(names(avdf[i]), "f2")) {
    paste(future2_start_year, "-",future2_end_year)
  } else {
    paste(baseline_start_year,"-",baseline_end_year)
  }
    csv_fileName = paste(AFB_Name, csv_scenario, csv_years, "AllDays", sep = '_')
    write_csv(csv, file = paste0(dir_output_csvs,csv_fileName,".csv", sep = ""))
    
    AllDays[[i]] = csv # new df for use with MonthSum
    names(AllDays)[i] = names(avdf[i])
}
```

```{r remove-extraneous-stuff}

# Remove unnecessary objects from the environment

# If for some reason the AllDays object does not work, you can use the script './Code/Misc/MonthSum.R' to run the MonthSum analyses using the csv output files one-by-one

rm(list=setdiff(ls(), c("AllDays", 
                        "AFB_Name", 
                        "model", 
                        "scenario1", 
                        "scenario2", 
                        "dir_output_csvs",
                        "years")))

```

## MonthSum

```{r MonthSum, message=FALSE}

monthSum <- list()

MACA = if_else(str_detect(model, "MACA"),
                     TRUE,
                     FALSE)

for(i in 1:length(AllDays)){
  df = AllDays[[i]]
  df = df %>%
    mutate(date = ymd(date)) %>%
    mutate(month = month(date)) %>%
    mutate(year = year(date)) %>%
    dplyr::select(-c(lat, lon))
  
  yearAvg = df %>%
    dplyr::select(!c('date','PPT_in', 'PPT_mm')) %>% # Exclude variables for which the result is not simply a monthly average
    dplyr::select(!(contains("days"))) %>%
    group_by(year, month) %>%
    summarise(across(where(is.double), mean))
  
  Abs_TminF = df %>%
  select(date, year, month, TMinF) %>%
  group_by(year, month) %>%
  summarise(Abs_TminF = min(TMinF))
  
  ppt = df %>%
  select(date, year, month, 'PPT_in', 'PPT_mm') %>%
  group_by(year, month) %>%
  summarise(across(contains('PPT'), sum))
  
  days = df %>%
  select(date, year, month, contains('days')) %>%
  group_by(year, month) %>%
  summarise(across(contains('days'), sum))
  
  all = yearAvg %>%
  full_join(days) %>%
  full_join(ppt) %>%
  full_join(Abs_TminF) %>%
  ungroup()
  
  monthAvg = all %>%
  dplyr::select(!year) %>%
  group_by(month) %>%
  summarise(across(everything(), mean)) %>%
  setNames(paste0('Avg_', names(.))) %>%
  rename(Abs_TminF = Avg_Abs_TminF) %>%
  select(Avg_month, # put in order on MonthSum csv
         Avg_PPT_in, 
         Avg_PPT_mm, 
         Avg_TMaxF, 
         Avg_TMinF, 
         Avg_TMeanF, 
         Abs_TminF, 
         any_of("Avg_hurs"),
         any_of("Avg_sfcWind"),
         Avg_GDDF,
         Avg_hotdays,
         Avg_colddays,
         Avg_wetdays,
         Avg_drydays,
         Avg_ftdays,
         )
  
  macaCols = if(MACA == "TRUE") {getMACAcols(all)} else {NULL} 
  
  monthAvg = if(MACA == "TRUE") {bind_cols(monthAvg, macaCols)} else {monthAvg}
  
  monthSum[[i]] = monthAvg # new df for export and use with DiffHist
  names(monthSum)[i] = names(AllDays[i])
}

# Create csv

for(i in 1:length(monthSum)){
  YrAverage = colMeans(monthSum[[i]][,2:ncol(monthSum[[i]])]) # converts columns to characters 
  YrTotals = colSums(monthSum[[i]][,2:ncol(monthSum[[i]])])
  csv = bind_rows(monthSum[[i]], YrAverage, YrTotals)
  
  # NA's 

  NAs = csv %>% # YrAverage
    filter(row_number() == 13) %>% # Because there will always be 12 months irrespective of model
    mutate(across(.cols = contains("PPT"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("days"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("Abs"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("GDDF"), ~na_if(.,.))) 
  
  NAs_Totals = csv %>% # YrTotals
    filter(row_number() == 14) %>%
    mutate(across(.cols = contains("Avg_T"), ~na_if(.,.))) %>%
    mutate(across(.cols = contains("Abs"), ~na_if(.,.))) %>%
    mutate(across(.cols = any_of("Avg_hurs"), ~na_if(.,.))) %>%
    mutate(across(.cols = any_of("Avg_sfcWind"), ~na_if(.,.))) 
    
  NAs_Totals = if(MACA == "TRUE") {mutate(across(.cols = .cols %in% colnames(macaCols), ~na_if(.,.)))
    } else {NAs_Totals}
  
  csv = csv %>%
    slice(1:(n()-2)) %>% # remove summary rows
    bind_rows(NAs, NAs_Totals) %>% # replace with NA's included
    rename(month = Avg_month) %>%
    mutate(month = as.character(month))
  
  csv[13,1] = "YrAverage"
  csv[14,1] = "YrTotals"
  
  monthSum[[i]] = csv # will write over original monthSum df that did not have final two columns
  names(monthSum)[i] = names(AllDays[i])
  }
  
# Save csv files 

for(i in 1:length(monthSum)){

csv_scenario = if (str_detect(names(monthSum[i]), "baseline")){
    "historical"
  } else if(str_detect(names(monthSum[i]),"s1")) {
    scenario1
  } else {
    scenario2
  }
  
  csv_years = if (str_detect(names(monthSum[i]), "f1")){
    paste(first(year(AllDays$results_s1f1$date)), "-", last(year(AllDays$results_s1f1$date)))
    
  } else if (str_detect(names(monthSum[i]), "f2")) {
    paste(first(year(AllDays$results_s1f2$date)),"-",last(year(AllDays$results_s1f2$date)))
  } else {
    paste(first(year(AllDays$results_baseline$date)),"-",last(year(AllDays$results_baseline$date)))
  }
    csv_fileName = paste(AFB_Name,csv_scenario,csv_years,"MonthSum",sep = '_')
    write_csv(monthSum[[i]], file = paste0(dir_output_csvs,csv_fileName,".csv",sep = ""))
}
```

```{r remove-objects}

rm(list=setdiff(ls(), c("monthSum", 
                        "AFB_Name", 
                        "model", 
                        "scenario1", 
                        "scenario2", 
                        "dir_output_csvs",
                        "years")))

```

## DiffHist

```{r DiffHist, warning=FALSE}

# input: monthSum dataframe 
# output: diffHist dataframe

diffHist <- list() # create diffHist dataframe for later use

# Create historical object

hist <- monthSum[[1]] # separate historical values

# Create dataframe

for(i in 2:length(monthSum)){ # 2 because [[1]] is historical 
  diff = hist %>%
  bind_rows(monthSum[[i]]) %>%
  group_by(month) %>%
  summarise_each(funs(diff(.))) %>% # this gives warnings because the function is deprecated, but it still works as of 5/9/23. If this breaks, look here first.
  #data.frame() %>%
  slice(1,5:12, 2:4,13:14) # arranges rows in the desired order
  
  diffHist[[i-1]] = diff # add to DiffHist dataframe for future use
  names(diffHist)[[i-1]] = names(monthSum[i])
}
  
# Save csv files 

for(i in 1:length(diffHist)){

csv_scenario = if (str_detect(names(diffHist[i]), "s1")){
    scenario1
  } else if(str_detect(names(diffHist[i]),"s2")) {
    scenario2
  } 
  
csv_years = if (str_detect(names(diffHist[i]), "f1")){
    paste0(years[3],"-",years[4])
  } else if (str_detect(names(diffHist[i]), "f2")) {
    paste0(years[5],"-",years[6])
  } 
    
csv_fileName = paste(AFB_Name,csv_scenario,csv_years,"DiffHist",sep = '_')
write_csv(diffHist[[i]], file = paste0(dir_output_csvs,csv_fileName,".csv",sep = ""))

}

```















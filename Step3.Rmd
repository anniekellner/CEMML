---
title: "Step3_test"
author: "Annie Kellner"
date: "2023-02-07"
output: html_document
---

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # 'echo' means R will print the code along with the results. 

rm(list = ls()) # clear workspace

library(terra)
library(sf)
library(dplyr)
library(stringr)
library(data.table)
library(tidyr)
library(viridis)
library(mapview)
library(raster)
library(lubridate)

source('./Code/CEMML/Misc/character_vectors.R')

```

# *PART ONE: User settings - choose AFB, model, scenarios, years of interest*

```{r set-directories}

dir_installation_boundaries <- "N:\\RStor\\mindyc\\afccm\\AF_CIP_ENV_Data_Phase3/Installation_Boundaries/" # site boundary shape files used for clipping

# The last two are straight from the original script. Will change so that directories can be set more generally in 'settings'

#dir_output_csvs = "N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Results_LOCA_V2" # output csv

#dir_functions = "N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Software Apps\\R scripts\\LOCA_V2" # scripts that call in functions for analysis
```



First, let's select which of the `models` you'd like to run. Highlight the word `models` and click 'Run' at the top right (or use the shortcut *Ctrl + Enter*). The available models correspond to numbers in the vector (e.g., HADGEM2-ES is [1]). 

In the code chunk below, enter the number between the brackets ([]) that corresponds with your model of interest. Then let's see what scenarios and variables are available for that model.

```{r enter-afb-and-model}

AFB_Name <- "Homestead_ARB" # Can create a vector similar to 'models' above if desired

model <- models[2] 

model_dir <- paste("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Data\\", model, sep = "")
scenario_dirs <- list.dirs(path = paste("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Data\\", model, sep = ""), full.names = FALSE)

cat(paste("The model you have selected is", model, "and the AFB is", AFB_Name))

```

The next chunk checks what scenarios (subfolders) are contained in the 'Data' folder associated with the model of interest.

```{r results='asis'}

cat("The scenarios (subfolders) associated with this model are:", sep = '\n') # cat() basically means 'print' in this context 
cat(paste("-", scenario_dirs), sep = "\n") # names into separate lines for readability ('\n' = line)

```

*Enter the scenarios of interest below:*

```{r enter-scenarios}

baseline <- "historical"
scenario1 <- "rcp45"
scenario2 <- "rcp85"

```

Next, let's see which variables are represented in this dataset

```{r}

baseline_dir = paste("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Data\\", model, "\\", baseline, sep = "")
baseline_filenames <- list.files(baseline_dir, pattern = '.nc', full.names = FALSE) # this line might need to be adjusted if some files are .ncdf or other

variables <- unique(substr(baseline_filenames, 1,6)) # Numbers 1 and 6 refer to character indices within the file names (i.e., the first six letters of the file name)

cat("The variables in this dataset are:", sep = '\n')
cat(paste("-", variables), sep = "\n")

```

```{r enter-years-of-interest}

baseline_start_year <- 1976 # Start year of interest for the historical time period  
baseline_end_year <- 2005 # End year of interest for the historical time period

future1_start_year <- 2026 # Start year of interest for the first future time period 
future1_end_year <- 2035 # End year of interest for the first future time period 

future2_start_year <- 2046 # Start year of interest for the second future time period 
future2_end_year <- 2055 # End year of interest for the second future time period 

```

####################################################################################################
# THIS CONCLUDES THE USER SETTINGS PORTION OF THE SCRIPT
# The rest of the script should be able to be run as-is...

# PART TWO: SUBSET FILES OF INTEREST

In this step, we are creating lists of filenames that we will eventually combine into raster stacks. WHen this chunk is finished running, you can check the objects to make sure 

```{r list-filenames}

scenarios <- c(baseline, scenario1, scenario2)

baseline_yrs <- paste(seq(baseline_start_year, baseline_end_year, 1), collapse = '|')
future1_yrs <- paste(seq(future1_start_year, future1_end_year, 1), collapse = '|')
future2_yrs <- paste(seq(future2_start_year, future2_end_year, 1), collapse = '|')

files_baseline <- list() # historical
files_s1f1 <- list() # scenario1, future 1
files_s1f2 <- list() # scenario 1, future 2
files_s2f1 <- list() # scenario 2, future 1
files_s2f2 <- list() # scenario 2, future 2

# List files for Baseline Scenario

for(i in 1:length(variables)){ 
  fileNames = list.files(baseline_dir, pattern = variables[i], full.names = TRUE)
  dt = data.table(fileNames, result = grepl(baseline_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_baseline[[i]] = dt2$fileNames
}

# List files for S1F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '\\')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f1[[i]] = dt2$fileNames
  }

# List files for S1F2

for(i in 1:length(variables)){ 
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f2[[i]] = dt2$fileNames
  }

# List files for S2F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario2, sep = '\\')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f1[[i]] = dt2$fileNames
}

# List files for S2F2

for(i in 1:length(variables)){ 
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f2[[i]] = dt2$fileNames
}

```

# PART THREE: SPATIAL CHECK

The next chunk checks that the spatial files are where they should be. The map will appear below the code chunk, and you can zoom in as needed to view small AFB's 

```{r spatial-check}

# AFB

afb_dir <- (paste(dir_installation_boundaries, AFB_Name, sep = '/'))

afbSF <- st_read(paste(afb_dir, '.shp', sep = "")) # read in using sf package first
afbSF <- sf::st_as_sf(afbSF, coords = c("longitude", "latitude"), crs = st_crs(4326)) # EPSG:4326 is lat/long

afb <- vect(afbSF) # for use with terra package

# Raster

stackBaseline_v1 <- rast(files_baseline[[1]]) # creates raster stack for first variable in files_baseline list
rx <- raster(stackBaseline_v1[[1]]) # just for plotting

# Quick plot

mapview(rx, col.regions = hcl.colors(5, palette = "viridis", alpha = 0.8)) + # First argument is arbitrary (# colors in palette)
  mapview(afbSF, col.regions = "black")

```

We need to check the size of the AFB to see whether we should use the centroid or multiple cells for analysis. If the results say 'matrix array', the script will use the centroid. If it says "spatVector", it will use the spatial object (afb).

```{r decision-to-use-shp-or-centroid-for-extraction, warning=FALSE, results='asis'}

# What is the cell size of the ncdf?

rx <- rast(rx) # convert to terra object because estimation is better. Could also use raster::area()
rxCrop <- terra::crop(rx, afb)
rxCell <- cellSize(rxCrop, unit = "km")
ncdf_cellSize <- mean(values(rxCell)) # get mean cell size of ncdf

# How big is the AFB?

afb_area <- expanse(afb, unit = "km")

centroid <- afbSF %>%
  st_centroid() %>%
  st_geometry() %>%
  st_coordinates()

# add an if_else statement to tell R whether to extract raster values from matrix or shapefile

extract_obj <- afb
cell <- cellFromXY(rx, centroid) # finds the cell (index) in which the centroid is located

extract_obj <- if(afb_area < ncdf_cellSize) cell else (afb)

cat("The object used for data extraction is ", class(extract_obj)) # 'numeric' = centroid (cell number); 
                                                                   # 'spatVector' = spatial object (multiple cells)

```


```{r extract-values}

# Create list for extracted values

# Historical

results_baseline <- list()
times_baseline <- vector() # Safest thing would be to make this a list and pull the dates for each variable separately, to make sure they're correct. 

results_s1f1 <- list()
times_s1f1 <- vector()

results_s1f2 <- list()
times_s1f2 <- vector()

system.time( # 43.9 minutes - AK home desktop
for(i in 1:length(files_baseline)){
  r = rast(files_baseline[[i]])
  #times_baseline = time(r)
  results_baseline[[i]] = r[extract_obj]
})

system.time( 
for(i in 1:length(files_s1f1)){
  r = rast(files_s1f1[[i]])
  #times_baseline = time(r)
  results_s1f1[[i]] = r[extract_obj]
})


############################################################
#saveRDS(results_baseline, file = './Results/baseline_raw.Rds') # 2/14/23

piv2 <- pr_test %>%
  mutate(PPT_mm = pr*86400) # From RasterUnitConvert function

```

```{r final-dataframes}

temp_baseline <- list()

for(i in 1:length(results_baseline)){
  temp_baseline[[i]] = pivot_longer(results_baseline[[i]],
                           cols = everything(),
                           names_to = "Layer",
                           names_prefix = variables[i],
                           values_to = variables[i],
                           values_drop_na = FALSE) 
                     
}

avdf_baseline <- tibble(lat = centroid[2], # allvaluesdf historical
                        lon = centroid[1], 
                        date = times_baseline, 
                        prcp = temp_baseline[[1]]$pr_day,
                        tmax = temp_baseline[[2]]$tasmax,
                        tmin = temp_baseline[[3]]$tasmin) 
```




```{r unit-conversion}

```



```{r check-results}

# Bring in results 

avdf <- read_csv('./Results/allvaluesdf.Rds')


piv2$Compare <- results$PPT_mm

piv2$Diff <- piv2$PPT_mm - piv2$Compare

sum(piv2$Diff)

```












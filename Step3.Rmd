---
title: "Step3_test"
author: "Annie Kellner"
date: "2023-02-07"
output: html_document
---

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # 'echo' means R will print the code along with the results. 

library(terra)
library(sf)
library(tidyverse)
library(data.table)
library(viridis)
library(mapview)
library(lubridate)
library(tmap)
library(tmaptools)
library(leaflet)

source('./Code/Misc/character_vectors.R')
source('./Code/CEMML/Functions/Function_General.R')

```

```{r clear-workspace}

#rm(list = ls())

```


*PART ONE: User settings - choose AFB, AFB manager, model, scenarios, years of interest*

If the AFB is managed by the Navy, set Navy = TRUE
If the AFB is managed by the Air Force, set Navy = FALSE

```{r set-directories}

# Is the AFB managed by the Navy?

Navy = FALSE


dir_installation_boundaries <- ifelse(Navy == FALSE, # site boundary (shapefile used for clipping)
                                       "N:\\RStor\\mindyc\\afccm\\AF_CIP_ENV_Data_Phase3/Installation_Boundaries/",
                                      "N:\\RStor\\mindyc\\afccm\\AF_CIP_ENV_Data_Phase3/Installation_Boundaries/NAVY/")
                                      

dir_output_csvs = './Results/Test/' 

```

First, let's select which of the `models` you'd like to run. Highlight the word `models` and click 'Run' at the top right (or use the shortcut *Ctrl + Enter*). The available models correspond to numbers in the vector (e.g., HADGEM2-ES is [1]). 

In the code chunk below, enter the number between the brackets ([]) that corresponds with your model of interest. Then let's see what scenarios and variables are available for that model.

```{r enter-afb-and-model}

AFB_Name <- "Homestead_ARB" # Can create a vector similar to 'models' above if desired

model <- models[2] 

cat(paste("The model you have selected is", model, "and the AFB is", AFB_Name))

```

The next chunk checks what scenarios (subfolders) are contained in the 'Data' folder associated with the model of interest.

```{r results='asis'}

# Directories

model_dir <- paste("N:\\RStor\\mindyc\\afccm\\Climate Modeling\\Data\\", model, sep = "") 


scenario_dirs <- list.dirs(path = model_dir, full.names = FALSE) 

cat("The scenarios (subfolders) associated with this model are:", sep = '\n') # cat() basically means 'print' in this context 
cat(paste("-", scenario_dirs), sep = "\n") # names into separate lines for readability ('\n' = line)

```

*Enter the scenarios of interest below:*

```{r enter-scenarios}

baseline <- "historical"
scenario1 <- "rcp45"
scenario2 <- "rcp85"

```

Next, let's see which variables are represented in this dataset

```{r}

baseline_dir = paste(model_dir, baseline, sep = "/")
baseline_filenames <- list.files(baseline_dir, pattern = '.nc', full.names = FALSE) # this line might need to be adjusted if some files are .ncdf or other

variables <- unique(substr(baseline_filenames, 1,6)) # Numbers 1 and 6 refer to character indices within the file names (i.e., the first six letters of the file name)

cat("The variables in this dataset are:", sep = '\n')
cat(paste("-", variables), sep = "\n")

```

Of the variables listed, which do you want to include in your analysis? If you want to include all variables, you can skip this step

```{r}

variables <- c("pr_day", "tasmax", "tasmin") # an example of how you would choose among the variables if desired

```


```{r enter-years-of-interest}

baseline_start_year <- 1985 # Start year of interest for the historical time period  
baseline_end_year <- 1995 # End year of interest for the historical time period

future1_start_year <- 2021 # Start year of interest for the first future time period 
future1_end_year <- 2031 # End year of interest for the first future time period 

future2_start_year <- 2051 # Start year of interest for the second future time period 
future2_end_year <- 2061 # End year of interest for the second future time period 

```

####################################################################################################
# THIS CONCLUDES THE USER SETTINGS PORTION OF THE SCRIPT
# The rest of the script should be able to be run as-is...

# PART TWO: SUBSET FILES OF INTEREST

In this step, we are creating lists of filenames that we will eventually combine into raster stacks. When this chunk is finished running, you can check the objects to make sure 

```{r list-filenames}

scenarios <- c(baseline, scenario1, scenario2)

baseline_yrs <- paste(seq(baseline_start_year, baseline_end_year, 1), collapse = '|')
future1_yrs <- paste(seq(future1_start_year, future1_end_year, 1), collapse = '|')
future2_yrs <- paste(seq(future2_start_year, future2_end_year, 1), collapse = '|')

files_baseline <- list() # historical
files_s1f1 <- list() # scenario1, future 1
files_s1f2 <- list() # scenario 1, future 2
files_s2f1 <- list() # scenario 2, future 1
files_s2f2 <- list() # scenario 2, future 2

# List files for Baseline Scenario

for(i in 1:length(variables)){ 
  fileNames = list.files(baseline_dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(baseline_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_baseline[[i]] = dt2$fileNames
}

# List files for S1F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f1[[i]] = dt2$fileNames
  }

# List files for S1F2

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario1, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s1f2[[i]] = dt2$fileNames
  }

# List files for S2F1 

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario2, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future1_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f1[[i]] = dt2$fileNames
}

# List files for S2F2

for(i in 1:length(variables)){ 
  dir = paste(model_dir, scenario2, sep = '/')
  fileNames = list.files(dir, pattern = variables[i], full.names = TRUE)
  fileNames = str_remove_all(fileNames, pattern = ".aux.xml")
  fileNames = fileNames[!duplicated(fileNames)]
  dt = data.table(fileNames, result = grepl(future2_yrs, fileNames))
  dt2 = dplyr::filter(dt, result == TRUE)
  files_s2f2[[i]] = dt2$fileNames
}

```

# PART THREE: SPATIAL CHECK

The next chunk checks that the spatial files are where they should be. The map will appear below the code chunk, and you can zoom in as needed to view small AFB's. You can press the 'show in new window' button (the white square with an arrow) to open the map in another page. You may have to wait a few moments for the map to produce. 

```{r spatial-check}

# Raster

stackX <- rast(files_s1f1[[1]]) # creates raster stack for first variable in s1f1 list (using s1f1 because has fewer files than baseline)

random_raster <- as.numeric(sample(1:nlyr(stackX), 1)) # get a random raster layer from StackX to use as an example
rx <- stackX[[random_raster]]

# AFB 

afb_dir <- (paste(dir_installation_boundaries, AFB_Name, sep = '/'))
afb_filename <- paste(afb_dir, '.shp', sep = "")

afbSF <- st_read(afb_filename) # Because plotting package (tmap) doesn't take terra vectors
afb <- vect(afb_filename)

# Crop single raster to AFB shape

rxCrop <- terra::crop(rx, afb)

# Quick interactive plot

#tmap_mode('view')

map <- tm_shape(as(rxCrop, "Raster")) + 
  tm_raster() + 
  tm_shape(afbSF) + 
  tm_borders()

lf <- tmap_leaflet(map)
lf

```

We should check the size of the AFB to see whether we should use its centroid or spatial extent to extract the data we need. If the AFB fits within a single NetCDF cell, it will be much faster to extract data using a single point.  

If the output of the following code chunk say 'matrix array', the script will use the centroid. If it says "spatVector", it will use the spatial object (afb).

```{r decision-to-use-shp-or-centroid-for-extraction, warning=FALSE, results='asis'}

# What is the cell size of the ncdf?

rxCellSize <- cellSize(rxCrop, unit = "km") # size of the cropped raster
ncdf_cellSize <- mean(values(rxCellSize)) # get mean cell size of ncdf

# How big is the AFB?

afb_area <- expanse(afb, unit = "km")

centroid <- geom(centroids(afb, inside = TRUE)) # inside = TRUE guarantees that the centroid is contained within the boundaries of the AFB (in some cases, depending on the shape of the AFB, the true centroid may be outside)
centroid_matrix <- cbind(centroid[3], centroid[4]) # used to find cell index. [3] and [4] refer to the values in the 'centroid' object. If a downstream error occurs, make sure these values are actually x and y.

# add an if_else statement to tell R whether to extract raster values from matrix or shapefile

extract_obj <- afb
cell <- terra::cellFromXY(rx, centroid_matrix) # finds the cell (index) in which the centroid is located

extract_obj <- if(afb_area < ncdf_cellSize) cell else (afb)

afbExt <- ext(afb)

cat("The object used for data extraction is ", class(extract_obj)) 

# Interpreting output: 
  
  #'numeric' = centroid (cell number) - this means the AFB did not extend beyond a single cell, so the centroid will be used for data extraction
  
  #'spatVector' = spatial object (multiple cells) - this means the AFB spanned multiple cells, so the polygon will be used for data extraction

```


```{r extract-values}

# Create lists for extracted values and vectors for extracting time (dates) from rasters

# NOTE: when it comes time to run a base that is large enough to span several raster cells, will need to amend this section. 
  # Will need to write a function into Functions.R that extracts data differently depending on whether we're using the centroid (point) or a spatial object (vect). terra::extract() could do either without writing a separate function, but it takes a lot longer to run for point data

# Historical

results_baseline <- list()
results_s1f1 <- list()
results_s1f2 <- list()
results_s2f1 <- list()
results_s2f2 <- list()

for(i in 1:length(files_baseline)){
  r = rast(files_baseline[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = values(crop(r, ext(afb)))
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) # do not use purrr:transpose
  colnames(tVals) = varName
  results_baseline[[i]] = tVals
  names(results_baseline)[i] = varName
  results_baseline[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s1f1)){
  r = rast(files_s1f1[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s1f1[[i]] = tVals
  names(results_s1f1)[i] = varName
  results_s1f1[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s1f2)){
  r = rast(files_s1f2[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s1f2[[i]] = tVals
  names(results_s1f2)[i] <- varName
  results_s1f2[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s2f1)){
  r = rast(files_s2f1[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s2f1[[i]] = tVals
  names(results_s2f1)[i] <- varName
  results_s2f1[[i]]$date = times
  rm(r)
}

for(i in 1:length(files_s2f2)){
  r = rast(files_s2f2[[i]])
  times = time(r)
  varName = varnames(r)[1]
  vals = r[extract_obj]
  tVals = data.table::transpose(vals) 
  colnames(tVals) = varName
  results_s2f2[[i]] = tVals
  names(results_s2f2)[i] <- varName
  results_s2f2[[i]]$date = times
  rm(r)
}

rm(list = ls(pattern = "^files")) # remove lists from the environment that aren't results

```

```{r allvaluesdf-dataframes}

# Equivalent of allvaluesdf (from original script)

lists <- Filter(function(x) is(x, "list"), mget(ls())) # reads in all remaining lists
nested_tibbles <- as_tibble(lists) # tibbles are tidy objects

avdf <- list() # results mimic allvaluesdf objects created by original script, with the exception of the 'time' column

for(i in 1:length(nested_tibbles)){
  sf_combo_nested = flatten(nested_tibbles[i]) # scenario-future combo
  sf = flatten(sf_combo_nested)
  tbl = tibble(lat = centroid_matrix[2], # will need to be changed when spatial objects are used for extraction
               lon = centroid_matrix[1],
               date = pluck(sf_combo_nested, 1, "date")) # pulls date column from first sublist
  sf_tbl = sf %>%
    as_tibble(sf, validate = NULL, .name_repair = "unique") %>% # validate argument necessary for running on cluster
    unnest(cols = where(is.numeric)) %>%
    dplyr::select(!(contains("date"))) %>%
    bind_cols(tbl) %>%
    dplyr::select(lat, lon, contains("max"), contains("min"), contains("pr"), date) 
  colnames(sf_tbl) = c("lat", "lon", "tmax", "tmin", "prcp", "date")
  avdf[[i]] = sf_tbl
  names(avdf)[i] = names(nested_tibbles[i]) 
}    
            
```


```{r add-derived-vars}

# Add derived variables to dataframes
# write to csv

for(i in 1:length(avdf)){
  csv = avdf[[i]]
  csv <- csv %>%
    mutate(PPT_mm = case_when( # if raster units are "kg-m-2 -1", convert to mm (otherwise keep as is)
      str_detect(units(rx), "kg") ~ prcp*86400,
      TRUE ~ as.numeric(prcp))) %>%
    mutate(PPT_in = RasterUnitConvert(PPT_mm, "MMtoIN")) %>%
    mutate(TMaxF = case_when(
      tmax > 200 ~ RasterUnitConvert(tmax, "KtoF"), # Because only values in Kelvin would be > 200
      TRUE ~ RasterUnitConvert(tmax, "CtoF"))) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMaxC = case_when(
      tmax > 200 ~ RasterUnitConvert(tmax, "KtoC"),
      TRUE ~ tmax)) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMinF = case_when(
      tmin > 200 ~ RasterUnitConvert(tmin, "KtoF"),
      TRUE ~ RasterUnitConvert(tmin, "CtoF"))) %>% # assuming if temp units are not Kelvin they are Celsius
    mutate(TMinC = case_when(
      tmin > 200 ~ RasterUnitConvert(tmin, "KtoC"),
      TRUE ~ tmin)) %>% # assuming if temp are not K they are C
    mutate(TMeanF = (TMaxF + TMinF)/2) %>%
    mutate(TmeanC = (TMaxC + TMinC)/2) %>%
    mutate(GDDF = RasterGDD(TMinF, TMaxF, 50, 86)) %>% # Threshold values of 50 and 86 come from original LOCASummmarize.R script
    mutate(hotdays = Rasterhotdays(TMaxC)) %>%
    mutate(colddays = Rastercolddays(TMinC)) %>%
    mutate(wetdays = Rasterwetdays(PPT_mm)) %>%
    mutate(ftDays = RasterFTdays(TMaxC, TMinC))
  csv_scenario = if (str_detect(names(avdf[i]), "baseline")){
    "historical"
  } else if(str_detect(names(avdf[i]),"s1")) {
    scenario1
  } else {
    scenario2
  }
  csv_years = if (str_detect(names(avdf[i]), "f1")){
    paste(future1_start_year, "-",future1_end_year)
  } else if (str_detect(names(avdf[i]), "f2")) {
    paste(future2_start_year, "-",future2_end_year)
  } else {
    paste(baseline_start_year,"-",baseline_end_year)
  }
    csv_fileName = paste(AFB_Name, csv_scenario, csv_years, "AllDays", sep = '_')
    write_csv(csv, file = paste(dir_output_csvs,csv_fileName,".csv", sep = ""))
}
```
















